import os
import pickle
import fitz  # PyMuPDF
import re
from sklearn.feature_extraction.text import TfidfVectorizer

BASE_DIR = os.path.dirname(__file__)
PDF_PATH = os.path.join(BASE_DIR, "math.pdf")
RAG_DIR = os.path.join(BASE_DIR, "rag_data")
os.makedirs(RAG_DIR, exist_ok=True)

def normalize_arabic(text):
    text = re.sub(r'[\u064B-\u065F]', '', text)
    text = re.sub(r'[Ø¥Ø£Ø¢]', 'Ø§', text)
    text = re.sub(r'Ù‰', 'ÙŠ', text)
    return text

def build_index():
    if not os.path.exists(PDF_PATH):
        print(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {PDF_PATH}")
        return

    print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù PDF...")
    doc = fitz.open(PDF_PATH)
    chunks = []
    
    for i, page in enumerate(doc):
        text = page.get_text()
        if len(text) > 50:
            raw_chunks = text.split('\n\n') 
            for chunk in raw_chunks:
                if len(chunk.strip()) > 30:
                    chunks.append({
                        "text": chunk.strip(),
                        "normalized": normalize_arabic(chunk),
                        "page": i + 1
                    })
    
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(chunks)} ÙÙ‚Ø±Ø©.")
    print("ğŸ§  Ø¨Ù†Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø¨Ø­Ø«...")
    vectorizer = TfidfVectorizer()
    corpus = [c['normalized'] for c in chunks]
    matrix = vectorizer.fit_transform(corpus)
    
    with open(os.path.join(RAG_DIR, "vectorizer.pkl"), "wb") as f: pickle.dump(vectorizer, f)
    with open(os.path.join(RAG_DIR, "tfidf_matrix.pkl"), "wb") as f: pickle.dump(matrix, f)
    with open(os.path.join(RAG_DIR, "chunks.pkl"), "wb") as f: pickle.dump(chunks, f)
        
    print("ğŸ‰ ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    build_index()